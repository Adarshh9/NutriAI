{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install -q sentence_transformers faiss-cpu huggingface_hub"
      ],
      "metadata": {
        "id": "2S95N1VOhpDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HDheTILFBSYS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwBmr7z2heym"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "df = pd.read_csv(\"/content/full_dataset.csv\")  # Replace with your dataset path\n",
        "\n",
        "# Step 2: Preprocess the data\n",
        "# Combine food and disease entities into a single text field\n",
        "df[\"food_disease_pair\"] = df[\"food_entity\"] + \" \" + df[\"disease_entity\"]\n",
        "\n",
        "# Create a combined label: 1 for recommend, -1 for avoid, 0 for neutral\n",
        "df[\"label\"] = df[\"is_treat\"] - df[\"is_cause\"]\n",
        "\n",
        "# Step 3: Generate embeddings\n",
        "# Load a pre-trained sentence embedding model (e.g., Sentence-BERT)\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Generate embeddings for food-disease pairs\n",
        "embeddings = model.encode(df[\"food_disease_pair\"].tolist())\n",
        "\n",
        "# Step 4: Build FAISS index\n",
        "# Convert embeddings to numpy array\n",
        "embeddings = np.array(embeddings).astype(\"float32\")\n",
        "\n",
        "# Initialize FAISS index\n",
        "dimension = embeddings.shape[1]  # Dimension of embeddings\n",
        "index = faiss.IndexFlatL2(dimension)  # L2 distance for similarity search\n",
        "\n",
        "# Add embeddings to the index\n",
        "index.add(embeddings)\n",
        "\n",
        "# Step 5: Save the FAISS index and preprocessed data\n",
        "faiss.write_index(index, \"food_disease_index.faiss\")\n",
        "df.to_csv(\"preprocessed_data.csv\", index=False)\n",
        "\n",
        "print(\"Training complete! FAISS index and preprocessed data saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# Step 1: Load preprocessed data and FAISS index\n",
        "df = pd.read_csv(\"preprocessed_data.csv\")  # Preprocessed dataset\n",
        "index = faiss.read_index(\"food_disease_index.faiss\")  # FAISS index\n",
        "\n",
        "# Step 2: Load the embedding model\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Step 3: Define the recommendation function\n",
        "def recommend_foods(disease: str, k: int = 5):\n",
        "    \"\"\"\n",
        "    Recommend foods to eat or avoid based on a disease.\n",
        "\n",
        "    Args:\n",
        "        disease (str): The disease input by the user.\n",
        "        k (int): Number of top recommendations to return.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing recommended and avoid foods.\n",
        "    \"\"\"\n",
        "    # Generate embedding for the query (disease)\n",
        "    query_embedding = model.encode([disease])\n",
        "\n",
        "    # Perform similarity search using FAISS\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "    # Get top recommendations\n",
        "    top_recommendations = df.iloc[indices[0]]\n",
        "\n",
        "    # Filter by label\n",
        "    recommend_foods = top_recommendations[top_recommendations[\"label\"] == 1][\"food_entity\"].tolist()\n",
        "    avoid_foods = top_recommendations[top_recommendations[\"label\"] == -1][\"food_entity\"].tolist()\n",
        "\n",
        "    return {\"recommend\": recommend_foods, \"avoid\": avoid_foods}\n"
      ],
      "metadata": {
        "id": "KhIR7hyUkcGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install groq"
      ],
      "metadata": {
        "id": "mDUcDq-flgGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "# Initialize the Groq client\n",
        "client = Groq(api_key='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')\n",
        "\n",
        "def generate_reasoning(disease: str, recommendations: dict):\n",
        "    \"\"\"\n",
        "    Generate reasoning and summary using Groq's API.\n",
        "\n",
        "    Args:\n",
        "        disease (str): The disease input by the user.\n",
        "        recommendations (dict): A dictionary containing recommended and avoid foods.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the reasoning and summary.\n",
        "    \"\"\"\n",
        "    # Prepare the prompt\n",
        "    prompt = (\n",
        "        f\"For a patient with {disease}, the system recommends eating {recommendations['recommend']} and avoiding {recommendations['avoid']}. \"\n",
        "        f\"Can you explain why these recommendations are made and provide additional medical advice? \"\n",
        "        f\"Please provide your reasoning inside <think> tags and the final summary/recommendations after the </think> tag.\"\n",
        "    )\n",
        "\n",
        "    # Generate the response using Groq's API\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"deepseek-r1-distill-llama-70b\",  # Use the desired model\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": (\n",
        "                    \"You are a helpful, respectful and honest medical assistant. \"\n",
        "                    \"Always answer as helpfully as possible, while being safe. \"\n",
        "                    \"Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \"\n",
        "                    \"Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. \"\n",
        "                    \"If you don’t know the answer to a question, please don’t share false information.\"\n",
        "                ),\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        temperature=0.6,  # Control the randomness of the output\n",
        "        max_tokens=4096,  # Maximum number of tokens to generate\n",
        "        top_p=0.95,  # Nucleus sampling parameter\n",
        "        stream=False,  # Set to False for a single response\n",
        "        stop=None,  # No specific stop tokens\n",
        "    )\n",
        "\n",
        "    # Extract the response\n",
        "    response = completion.choices[0].message.content\n",
        "\n",
        "    # Split the response into reasoning and summary\n",
        "    if \"<think>\" in response and \"</think>\" in response:\n",
        "        reasoning = response.split(\"<think>\")[1].split(\"</think>\")[0].strip()\n",
        "        summary = response.split(\"</think>\")[1].strip()\n",
        "    else:\n",
        "        reasoning = \"No reasoning provided.\"\n",
        "        summary = response.strip()\n",
        "\n",
        "    return {\"reasoning\": reasoning, \"summary\": summary}"
      ],
      "metadata": {
        "id": "meRPJLbUo8y5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Get recommendations\n",
        "disease = \"diabetes\"\n",
        "recommendations = recommend_foods(disease)\n",
        "\n",
        "# Step 2: Generate reasoning and summary using Groq's API\n",
        "result = generate_reasoning(disease, recommendations)\n",
        "\n",
        "# Step 3: Display results\n",
        "print(f\"For {disease}:\")\n",
        "print(\"- Foods to Eat:\", recommendations[\"recommend\"])\n",
        "print(\"- Foods to Avoid:\", recommendations[\"avoid\"])\n",
        "print(\"\\nReasoning:\")\n",
        "print(result[\"reasoning\"])\n",
        "print(\"\\nSummary and Recommendations:\")\n",
        "print(result[\"summary\"])"
      ],
      "metadata": {
        "id": "CaSU67t5mhZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R8lBth60mux1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q streamlit"
      ],
      "metadata": {
        "id": "Rzp5l6rBtMXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from groq import Groq\n",
        "import streamlit as st\n",
        "\n",
        "# Set up the Streamlit app\n",
        "st.set_page_config(page_title=\"NutriMumbai AI\", page_icon=\"🍏\", layout=\"wide\")\n",
        "\n",
        "# Custom CSS for styling\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "    <style>\n",
        "    .stApp {\n",
        "        background-color: #f5f5f5;\n",
        "    }\n",
        "    .stButton>button {\n",
        "        background-color: #4CAF50;\n",
        "        color: white;\n",
        "        font-size: 16px;\n",
        "        padding: 10px 24px;\n",
        "        border-radius: 8px;\n",
        "    }\n",
        "    .stTextInput>div>div>input {\n",
        "        font-size: 16px;\n",
        "        padding: 10px;\n",
        "    }\n",
        "    .stMarkdown h1 {\n",
        "        color: #4CAF50;\n",
        "    }\n",
        "    .stMarkdown h2 {\n",
        "        color: #2E86C1;\n",
        "    }\n",
        "    .stMarkdown h3 {\n",
        "        color: #2E86C1;\n",
        "    }\n",
        "    </style>\n",
        "    \"\"\",\n",
        "    unsafe_allow_html=True,\n",
        ")\n",
        "\n",
        "# Title and description\n",
        "st.title(\"🍏 NutriMumbai AI\")\n",
        "st.markdown(\"Your AI-powered dietary companion for managing diseases and staying healthy in Mumbai.\")\n",
        "st.markdown(\"---\")\n",
        "\n",
        "# Step 1: Load preprocessed data and FAISS index\n",
        "@st.cache_resource\n",
        "def load_data_and_model():\n",
        "    df = pd.read_csv(\"preprocessed_data.csv\")  # Preprocessed dataset\n",
        "    index = faiss.read_index(\"food_disease_index.faiss\")  # FAISS index\n",
        "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Embedding model\n",
        "    return df, index, model\n",
        "\n",
        "df, index, model = load_data_and_model()\n",
        "\n",
        "# Step 2: Define the recommendation function\n",
        "def recommend_foods(disease: str, k: int = 5):\n",
        "    \"\"\"\n",
        "    Recommend foods to eat or avoid based on a disease.\n",
        "\n",
        "    Args:\n",
        "        disease (str): The disease input by the user.\n",
        "        k (int): Number of top recommendations to return.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing recommended and avoid foods.\n",
        "    \"\"\"\n",
        "    # Generate embedding for the query (disease)\n",
        "    query_embedding = model.encode([disease])\n",
        "\n",
        "    # Perform similarity search using FAISS\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "    # Get top recommendations\n",
        "    top_recommendations = df.iloc[indices[0]]\n",
        "\n",
        "    # Filter by label\n",
        "    recommend_foods = top_recommendations[top_recommendations[\"label\"] == 1][\"food_entity\"].tolist()\n",
        "    avoid_foods = top_recommendations[top_recommendations[\"label\"] == -1][\"food_entity\"].tolist()\n",
        "\n",
        "    return {\"recommend\": recommend_foods, \"avoid\": avoid_foods}\n",
        "\n",
        "# Step 3: Initialize the Groq client\n",
        "client = Groq(api_key='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')\n",
        "\n",
        "# Step 4: Define the reasoning function\n",
        "def generate_reasoning(disease: str, recommendations: dict):\n",
        "    \"\"\"\n",
        "    Generate reasoning and summary using Groq's API.\n",
        "\n",
        "    Args:\n",
        "        disease (str): The disease input by the user.\n",
        "        recommendations (dict): A dictionary containing recommended and avoid foods.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the reasoning and summary.\n",
        "    \"\"\"\n",
        "    # Prepare the prompt\n",
        "    prompt = (\n",
        "        f\"For a patient with {disease}, the system recommends eating {recommendations['recommend']} and avoiding {recommendations['avoid']}. \"\n",
        "        f\"Can you explain why these recommendations are made and provide additional medical advice? \"\n",
        "        f\"Please provide your reasoning inside <think> tags and the final summary/recommendations after the </think> tag.\"\n",
        "    )\n",
        "\n",
        "    # Generate the response using Groq's API\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"deepseek-r1-distill-llama-70b\",  # Use the desired model\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": (\n",
        "                    \"You are a helpful, respectful and honest medical assistant. \"\n",
        "                    \"Always answer as helpfully as possible, while being safe. \"\n",
        "                    \"Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \"\n",
        "                    \"Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. \"\n",
        "                    \"If you don’t know the answer to a question, please don’t share false information.\"\n",
        "                ),\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": prompt},\n",
        "        ],\n",
        "        temperature=0.6,  # Control the randomness of the output\n",
        "        max_tokens=4096,  # Maximum number of tokens to generate\n",
        "        top_p=0.95,  # Nucleus sampling parameter\n",
        "        stream=False,  # Set to False for a single response\n",
        "        stop=None,  # No specific stop tokens\n",
        "    )\n",
        "\n",
        "    # Extract the response\n",
        "    response = completion.choices[0].message.content\n",
        "\n",
        "    # Split the response into reasoning and summary\n",
        "    if \"<think>\" in response and \"</think>\" in response:\n",
        "        reasoning = response.split(\"<think>\")[1].split(\"</think>\")[0].strip()\n",
        "        summary = response.split(\"</think>\")[1].strip()\n",
        "    else:\n",
        "        reasoning = \"No reasoning provided.\"\n",
        "        summary = response.strip()\n",
        "\n",
        "    return {\"reasoning\": reasoning, \"summary\": summary}\n",
        "\n",
        "# Step 5: Streamlit UI\n",
        "st.sidebar.title(\"Settings\")\n",
        "disease = st.sidebar.text_input(\"Enter the disease (e.g., diabetes):\", \"diabetes\")\n",
        "k = st.sidebar.slider(\"Number of recommendations:\", min_value=1, max_value=10, value=5)\n",
        "\n",
        "# Step 6: Get recommendations and reasoning\n",
        "if st.sidebar.button(\"Get Recommendations\"):\n",
        "    with st.spinner(\"Generating recommendations...\"):\n",
        "        recommendations = recommend_foods(disease, k)\n",
        "        result = generate_reasoning(disease, recommendations)\n",
        "\n",
        "    # Display results\n",
        "    st.subheader(f\"Recommendations for {disease}:\")\n",
        "    col1, col2 = st.columns(2)\n",
        "    with col1:\n",
        "        st.markdown(\"### 🍎 Foods to Eat\")\n",
        "        for food in recommendations[\"recommend\"]:\n",
        "            st.markdown(f\"- {food}\")\n",
        "    with col2:\n",
        "        st.markdown(\"### 🚫 Foods to Avoid\")\n",
        "        for food in recommendations[\"avoid\"]:\n",
        "            st.markdown(f\"- {food}\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"Reasoning\")\n",
        "    st.markdown(result[\"reasoning\"])\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"Summary and Recommendations\")\n",
        "    st.markdown(result[\"summary\"])\n",
        "\n",
        "# Footer\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"Built with ❤️ by NutriMumbai AI\")"
      ],
      "metadata": {
        "id": "vU3Ryo6ctJpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q139e_GDBV82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Knowledge Graph Based Model"
      ],
      "metadata": {
        "id": "vs8TGgJ5BXWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/full_dataset.csv\")\n",
        "\n",
        "# Create an empty list to store relationships\n",
        "relationships = []\n",
        "\n",
        "# Iterate through the dataset and extract relationships\n",
        "for index, row in df.iterrows():\n",
        "    food = row[\"food_entity\"]\n",
        "    disease = row[\"disease_entity\"]\n",
        "\n",
        "    if row[\"is_cause\"] == 1:\n",
        "        relationships.append((food, \"causes\", disease))\n",
        "    if row[\"is_treat\"] == 1:\n",
        "        relationships.append((food, \"treats\", disease))\n",
        "\n",
        "# Convert the list into a DataFrame\n",
        "kg_df = pd.DataFrame(relationships, columns=[\"source\", \"relationship\", \"target\"])\n",
        "\n",
        "kg_df"
      ],
      "metadata": {
        "id": "L7LVnu-xBV4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kg_df.to_csv(\"knowledge_graph_relationships.csv\", index=False)"
      ],
      "metadata": {
        "id": "mORdj-aHCwr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install networkx"
      ],
      "metadata": {
        "id": "ICxxB7WYCwod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Create a directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Add nodes and edges\n",
        "for index, row in kg_df.iterrows():\n",
        "    G.add_edge(row[\"source\"], row[\"target\"], relationship=row[\"relationship\"])\n",
        "\n",
        "# Visualize the graph (optional)\n",
        "import matplotlib.pyplot as plt\n",
        "nx.draw(G, with_labels=True, font_weight=\"bold\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pltqxr6BCwk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G"
      ],
      "metadata": {
        "id": "2RH1lNS5GNPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "# Save the graph to a .pkl file\n",
        "with open(\"knowledge_graph.pkl\", \"wb\") as f:\n",
        "    pickle.dump(G, f)\n",
        "\n",
        "print(\"Knowledge graph saved as 'knowledge_graph.pkl'\")"
      ],
      "metadata": {
        "id": "WFHCSjJkGNGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all foods that treat diabetes\n",
        "for source, target, data in G.edges(data=True):\n",
        "    if data[\"relationship\"] == \"treats\" and target == \"diabetes\":\n",
        "        print(source)"
      ],
      "metadata": {
        "id": "YhYNgXnHCwhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all foods that cause diabetes\n",
        "for source, target, data in G.edges(data=True):\n",
        "    if data[\"relationship\"] == \"causes\" and target == \"diabetes\":\n",
        "        print(source)"
      ],
      "metadata": {
        "id": "WljCGwKuCweW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EArgBXUzCwaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DB2zdhxYCwXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M2r8-7ZuCwT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M4QNzMF3CwQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XfvHYFlLCwNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ltKHdA4tCwJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KyaWW2HOCwB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lf-gmh4xCv9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "yZLt26ZbBncL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "Vy2airH9By6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['is_cause'==0]]"
      ],
      "metadata": {
        "id": "yRFmhoDnCMKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FF7jEki_CdcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracted Features From Raw Dataset"
      ],
      "metadata": {
        "id": "c7xqV2xVMEwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/gjorgjinac/food-disease-dataset.git"
      ],
      "metadata": {
        "id": "9tzR0QIMMDzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the path to the dataset\n",
        "dataset_path = \"/content/food-disease-dataset/splits\"\n",
        "\n",
        "# Initialize empty lists to store data\n",
        "all_train_data = []\n",
        "all_val_data = []\n",
        "all_test_data = []\n",
        "\n",
        "# Combine data from cause_folds\n",
        "cause_folds_path = os.path.join(dataset_path, \"cause_folds\")\n",
        "for fold in range(0, 10):  # Assuming there are 10 folds\n",
        "    fold_path = os.path.join(cause_folds_path, f\"fold{fold}\")\n",
        "    train_data = pd.read_csv(os.path.join(fold_path, \"train.csv\"))\n",
        "    val_data = pd.read_csv(os.path.join(fold_path, \"val.csv\"))\n",
        "    test_data = pd.read_csv(os.path.join(fold_path, \"test.csv\"))\n",
        "    all_train_data.append(train_data)\n",
        "    all_val_data.append(val_data)\n",
        "    all_test_data.append(test_data)\n",
        "\n",
        "# Combine data from treat_folds\n",
        "treat_folds_path = os.path.join(dataset_path, \"treat_folds\")\n",
        "for fold in range(0, 10):  # Assuming there are 10 folds\n",
        "    fold_path = os.path.join(treat_folds_path, f\"fold{fold}\")\n",
        "    train_data = pd.read_csv(os.path.join(fold_path, \"train.csv\"))\n",
        "    val_data = pd.read_csv(os.path.join(fold_path, \"val.csv\"))\n",
        "    test_data = pd.read_csv(os.path.join(fold_path, \"test.csv\"))\n",
        "    all_train_data.append(train_data)\n",
        "    all_val_data.append(val_data)\n",
        "    all_test_data.append(test_data)\n",
        "\n",
        "# Combine all data into single DataFrames\n",
        "combined_train_data = pd.concat(all_train_data, ignore_index=True)\n",
        "combined_val_data = pd.concat(all_val_data, ignore_index=True)\n",
        "combined_test_data = pd.concat(all_test_data, ignore_index=True)\n",
        "\n",
        "# Save the combined datasets (optional)\n",
        "combined_train_data.to_csv(\"combined_train_data.csv\", index=False)\n",
        "combined_val_data.to_csv(\"combined_val_data.csv\", index=False)\n",
        "combined_test_data.to_csv(\"combined_test_data.csv\", index=False)"
      ],
      "metadata": {
        "id": "99vqkt5vMDux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Select features and target for training\n",
        "train_features = combined_train_data[[\n",
        "    \"bert_cause_cs_pairs\", \"bert_treat_cs_pairs\",\n",
        "    \"roberta_cause_cs_pairs\", \"roberta_treat_cs_pairs\",\n",
        "    \"biobert_cause_cs_pairs\", \"biobert_treat_cs_pairs\"\n",
        "]]\n",
        "train_target = combined_train_data[\"is_cause\"]  # or combined_train_data[\"is_treat\"]\n",
        "\n",
        "# Train a Random Forest classifier\n",
        "clf = RandomForestClassifier(random_state=42)\n",
        "clf.fit(train_features, train_target)\n"
      ],
      "metadata": {
        "id": "_54kEqgRMq0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on validation data\n",
        "val_features = combined_val_data[[\n",
        "    \"bert_cause_cs_pairs\", \"bert_treat_cs_pairs\",\n",
        "    \"roberta_cause_cs_pairs\", \"roberta_treat_cs_pairs\",\n",
        "    \"biobert_cause_cs_pairs\", \"biobert_treat_cs_pairs\"\n",
        "]]\n",
        "val_target = combined_val_data[\"is_cause\"]  # or combined_val_data[\"is_treat\"]\n",
        "\n",
        "val_pred = clf.predict(val_features)\n",
        "print(\"Validation Results:\")\n",
        "print(classification_report(val_target, val_pred))\n"
      ],
      "metadata": {
        "id": "WsGWASMsMT0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluate on test data\n",
        "test_features = combined_test_data[[\n",
        "    \"bert_cause_cs_pairs\", \"bert_treat_cs_pairs\",\n",
        "    \"roberta_cause_cs_pairs\", \"roberta_treat_cs_pairs\",\n",
        "    \"biobert_cause_cs_pairs\", \"biobert_treat_cs_pairs\"\n",
        "]]\n",
        "test_target = combined_test_data[\"is_cause\"]  # or combined_test_data[\"is_treat\"]\n",
        "\n",
        "test_pred = clf.predict(test_features)\n",
        "print(\"Test Results:\")\n",
        "print(classification_report(test_target, test_pred))"
      ],
      "metadata": {
        "id": "AUA9VlZjM0J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the Random Forest model\n",
        "with open(\"rf_model.pkl\", \"wb\") as f:\n",
        "  pickle.dump(clf, f)"
      ],
      "metadata": {
        "id": "OYHnJUiTM1qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "# Load the trained model\n",
        "# clf = joblib.load(\"random_forest_model.pkl\")\n",
        "with open(\"rf_model.pkl\", \"rb\") as f:\n",
        "  clf = pickle.load( f)\n",
        "\n",
        "# Example: User inputs a disease\n",
        "user_disease = \"asthma\"\n",
        "\n",
        "combined_train_data = pd.read_csv('combined_train_data.csv')\n",
        "# Get all food-disease pairs for the user's disease\n",
        "food_disease_pairs = combined_train_data[combined_train_data[\"term2\"] == user_disease]\n",
        "\n",
        "# Select features for inference\n",
        "inference_features = food_disease_pairs[[\n",
        "    \"bert_cause_cs_pairs\", \"bert_treat_cs_pairs\",\n",
        "    \"roberta_cause_cs_pairs\", \"roberta_treat_cs_pairs\",\n",
        "    \"biobert_cause_cs_pairs\", \"biobert_treat_cs_pairs\"\n",
        "]]\n",
        "\n",
        "# Predict relationships\n",
        "predictions = clf.predict(inference_features)\n",
        "\n",
        "# Add predictions to the DataFrame\n",
        "food_disease_pairs[\"prediction\"] = predictions\n",
        "\n",
        "# Generate recommendations\n",
        "recommend_foods = food_disease_pairs[food_disease_pairs[\"prediction\"] == 0][\"term1\"].tolist()\n",
        "avoid_foods = food_disease_pairs[food_disease_pairs[\"prediction\"] == 1][\"term1\"].tolist()\n",
        "\n",
        "# Display recommendations\n",
        "print(f\"For {user_disease}:\")\n",
        "print(\"- Foods to Eat:\", recommend_foods)\n",
        "print(\"- Foods to Avoid:\", avoid_foods)"
      ],
      "metadata": {
        "id": "34CqpM4ZM5If"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip list"
      ],
      "metadata": {
        "id": "XIxpPXIlM8z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia-api\n"
      ],
      "metadata": {
        "id": "aGaiuQhIa7BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "wiki_wiki = wikipediaapi.Wikipedia(user_agent='MyProjectName (merlin@example.com)', language='en')\n",
        "\n",
        "page_py = wiki_wiki.page('Python_(programming_language)')"
      ],
      "metadata": {
        "id": "ebNHY506kdvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page_py = wiki_wiki.page('Asthma')\n",
        "print(\"Page - Exists: %s\" % page_py.exists())\n",
        "# Page - Exists: True\n",
        "\n",
        "page_missing = wiki_wiki.page('NonExistingPageWithStrangeName')\n",
        "print(\"Page - Exists: %s\" %     page_missing.exists())\n",
        "# Page - Exists: False"
      ],
      "metadata": {
        "id": "3nQFl9fokkbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_wiki = wikipediaapi.Wikipedia(\n",
        "    user_agent='MyProjectName (merlin@example.com)',\n",
        "    language='en',\n",
        "    extract_format=wikipediaapi.ExtractFormat.WIKI\n",
        ")\n",
        "\n",
        "p_wiki = wiki_wiki.page(\"Asthma\")\n",
        "print(p_wiki.text)\n",
        "\n"
      ],
      "metadata": {
        "id": "wLWUoSnykunU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sg3tOMM-lkle"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}